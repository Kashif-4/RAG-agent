{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu6htyAo8_RW",
        "outputId": "b5828e6b-b4d8-4070-8039-68bb34b00fdc"
      },
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client cohere python-dotenv requests tqdm PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y1vfU2j-OuX",
        "outputId": "9df497bd-49d4-4909-c8eb-e438bc78a735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Connected to local Qdrant\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "\n",
        "# ✅ Connect to local Qdrant (no API key needed)\n",
        "client = QdrantClient(host=\"localhost\", port=6333)\n",
        "print(\"✅ Connected to local Qdrant\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POo0tnz_iQLV",
        "outputId": "a8ec7c42-eb17-45e1-89f1-cea7ca60ab90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wOlJV5zmFzq"
      },
      "outputs": [],
      "source": [
        "# BASE_URL = \"https://data.lhc.gov.pk/reported_judgments/judgments_approved_for_reporting\"\n",
        "# PDF_DIR = \"lhc_pdfs\"\n",
        "# TEXT_DIR = \"extracted_texts\"\n",
        "\n",
        "# os.makedirs(PDF_DIR, exist_ok=True)\n",
        "# os.makedirs(TEXT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ESWG9fLmkRq",
        "outputId": "d7c2219e-f9d3-489e-83be-224b2a410cbe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create target folder if it doesn't exist\n",
        "os.makedirs(\"extracted_texts\", exist_ok=True)\n",
        "\n",
        "# Move all .txt files from the current directory to extracted_texts/\n",
        "for filename in os.listdir():\n",
        "    if filename.endswith(\".txt\"):\n",
        "        source = filename\n",
        "        destination = os.path.join(\"extracted_texts\", filename)\n",
        "        shutil.move(source, destination)\n",
        "        print(f\"Moved {filename} to extracted_texts/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ll3aXPOpmzol"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def chunk_text(text, max_tokens=500):\n",
        "    sentences = text.split(\". \")\n",
        "    chunks, chunk = [], \"\"\n",
        "    for sentence in sentences:\n",
        "        if len((chunk + sentence).split()) > max_tokens:\n",
        "            chunks.append(chunk.strip())\n",
        "            chunk = sentence\n",
        "        else:\n",
        "            chunk += sentence + \". \"\n",
        "    if chunk:\n",
        "        chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "texts_dir = Path(\"extracted_texts\")\n",
        "all_chunks = []\n",
        "\n",
        "for file in texts_dir.glob(\"*.txt\"):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    chunks = chunk_text(text)\n",
        "    for chunk in chunks:\n",
        "        all_chunks.append({\"text\": chunk, \"source\": file.name})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD9KQ6-wnRA3",
        "outputId": "98989305-811f-49ba-b6a9-2dc9e097685e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total text chunks: 499\n"
          ]
        }
      ],
      "source": [
        "print(f\"✅ Total text chunks: {len(all_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkYRritTEg4_",
        "outputId": "3c47ceb4-3a1c-4034-e6a1-751d92f9ec18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cohere client initialized\n"
          ]
        }
      ],
      "source": [
        "import cohere\n",
        "\n",
        "cohere_api_key = \"ifZ7uhK8Va1D0BsVBNUbwtH2dysmkph4j9RHbvX8\"\n",
        "co = cohere.Client(cohere_api_key)\n",
        "\n",
        "print(\"✅ Cohere client initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "gE7JeL6tnenZ",
        "outputId": "ad4c51ab-8fdb-43a9-b95e-c6cb25e807db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created collection: lhc_judgments\n"
          ]
        }
      ],
      "source": [
        "collection_name = \"lhc_judgments\"\n",
        "\n",
        "if client.collection_exists(collection_name=collection_name):\n",
        "    client.delete_collection(collection_name=collection_name)\n",
        "    print(f\"🗑️ Deleted existing collection: {collection_name}\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(\n",
        "        size=384,\n",
        "        distance=Distance.COSINE\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"✅ Created collection: {collection_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHlyToPnekA",
        "outputId": "df8cd247-8957-45ca-abff-3d7b0c724f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📤 Uploaded batch 1\n",
            "📤 Uploaded batch 2\n",
            "📤 Uploaded batch 3\n",
            "📤 Uploaded batch 4\n",
            "📤 Uploaded batch 5\n",
            "📤 Uploaded batch 6\n",
            "📤 Uploaded batch 7\n",
            "📤 Uploaded batch 8\n",
            "📤 Uploaded batch 9\n",
            "📤 Uploaded batch 10\n",
            "📤 Uploaded batch 11\n",
            "📤 Uploaded batch 12\n",
            "📤 Uploaded batch 13\n",
            "📤 Uploaded batch 14\n",
            "📤 Uploaded batch 15\n",
            "📤 Uploaded batch 16\n",
            "📤 Uploaded batch 17\n",
            "📤 Uploaded batch 18\n",
            "📤 Uploaded batch 19\n",
            "📤 Uploaded batch 20\n",
            "📤 Uploaded batch 21\n",
            "📤 Uploaded batch 22\n",
            "📤 Uploaded batch 23\n",
            "📤 Uploaded batch 24\n",
            "📤 Uploaded batch 25\n",
            "📤 Uploaded batch 26\n",
            "📤 Uploaded batch 27\n",
            "📤 Uploaded batch 28\n",
            "📤 Uploaded batch 29\n",
            "⚠️ Rate limit. Retrying in 10s...\n",
            "📤 Uploaded batch 30\n",
            "📤 Uploaded batch 31\n",
            "📤 Uploaded batch 32\n",
            "📤 Uploaded batch 33\n",
            "📤 Uploaded batch 34\n",
            "📤 Uploaded batch 35\n",
            "📤 Uploaded batch 36\n",
            "📤 Uploaded batch 37\n",
            "📤 Uploaded batch 38\n",
            "📤 Uploaded batch 39\n",
            "📤 Uploaded batch 40\n",
            "📤 Uploaded batch 41\n",
            "📤 Uploaded batch 42\n",
            "📤 Uploaded batch 43\n",
            "📤 Uploaded batch 44\n",
            "📤 Uploaded batch 45\n",
            "📤 Uploaded batch 46\n",
            "📤 Uploaded batch 47\n",
            "📤 Uploaded batch 48\n",
            "📤 Uploaded batch 49\n",
            "⚠️ Rate limit. Retrying in 10s...\n",
            "📤 Uploaded batch 50\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "import time\n",
        "from qdrant_client.models import PointStruct\n",
        "from datetime import datetime\n",
        "\n",
        "def get_embeddings_with_retry(texts, retries=10, delay=10):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return co.embed(\n",
        "                texts=texts,\n",
        "                model=\"embed-english-light-v3.0\",\n",
        "                input_type=\"search_document\"\n",
        "            ).embeddings\n",
        "        except Exception as e:\n",
        "            if hasattr(e, \"status_code\") and e.status_code == 429:\n",
        "                print(f\"⚠️ Rate limit. Retrying in {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "            elif \"rate limit\" in str(e).lower():\n",
        "                print(f\"⚠️ Retry due to rate limit: {e}\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                raise e\n",
        "    raise RuntimeError(\"❌ Failed after multiple retries.\")\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "for i in range(0, len(all_chunks), batch_size):\n",
        "    batch = all_chunks[i:i+batch_size]\n",
        "    texts = [x[\"text\"] for x in batch]\n",
        "    embeddings = get_embeddings_with_retry(texts)\n",
        "\n",
        "    points = [\n",
        "        PointStruct(\n",
        "            id=str(uuid.uuid4()),\n",
        "            vector=emb,\n",
        "            payload={\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"source\": chunk[\"source\"],\n",
        "                \"url\": f\"https://data.lhc.gov.pk/reported_judgments/judgments_approved_for_reporting/{chunk['source'].replace('.txt', '.pdf')}\",\n",
        "                \"updated_at\": datetime.now().strftime(\"%Y-%m-%d\")\n",
        "            }\n",
        "        )\n",
        "        for emb, chunk in zip(embeddings, batch)\n",
        "    ]\n",
        "\n",
        "    client.upsert(collection_name=collection_name, points=points)\n",
        "    print(f\"📤 Uploaded batch {i // batch_size + 1}\")\n",
        "    time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEdwNv_qsILp",
        "outputId": "dbf25731-847b-4ee9-d733-48ad4c8c09fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔐 Using API key: gsk_j ...\n"
          ]
        }
      ],
      "source": [
        "print(\"🔐 Using API key:\", groq_api_key[:5], \"...\")  # Shows only first 5 chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T3BJRTRUBXbZ"
      },
      "outputs": [],
      "source": [
        "def search_query(query, client, collection_name, co, top_k=3):\n",
        "    query_embed = co.embed(\n",
        "        texts=[query],\n",
        "        model=\"embed-english-light-v3.0\",\n",
        "        input_type=\"search_query\"\n",
        "    ).embeddings[0]\n",
        "\n",
        "    results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embed,\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "\n",
        "    return [hit.payload.get(\"text\", \"\") for hit in results]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GBs9T-3eBfRk"
      },
      "outputs": [],
      "source": [
        "def rerank_context(query, context_list, co):\n",
        "    if not context_list:\n",
        "        print(\"⚠️ No context provided for reranking.\")\n",
        "        return []\n",
        "\n",
        "    results = co.rerank(\n",
        "        query=query,\n",
        "        documents=context_list,\n",
        "        top_n=min(5, len(context_list)),\n",
        "        model=\"rerank-english-v3.0\"\n",
        "    )\n",
        "\n",
        "    print(\"✅ Got rerank results:\", results)\n",
        "    if results:\n",
        "        print(\"👀 Sample item:\", results[0])\n",
        "    else:\n",
        "        print(\"⚠️ Rerank results is empty.\")\n",
        "\n",
        "    return [doc[\"text\"] if isinstance(doc, dict) else doc for _, doc in results]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LTyCRh-AACQT"
      },
      "outputs": [],
      "source": [
        "def query_rewriter(conversation_history, groq_api_key):\n",
        "    prompt = f\"\"\"You are an AI assistant. Rewrite the following conversation into a concise and clear search query:\n",
        "\n",
        "    Conversation:\n",
        "    {conversation_history}\n",
        "\n",
        "    Rewritten Query:\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {groq_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": \"llama3-8b-8192\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "        headers=headers,\n",
        "        json=data\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception:\n",
        "        print(\"❌ Query rewriting failed:\", response.status_code, response.text)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cnlrrCMvAQLZ"
      },
      "outputs": [],
      "source": [
        "# # Simulate conversation\n",
        "# conversation = f\"User: {question}\"  # You can expand this if you track full history\n",
        "\n",
        "# # Rewrite query\n",
        "# rewritten_query = query_rewriter(conversation, groq_api_key)\n",
        "# print(\"📝 Rewritten Query:\", rewritten_query)\n",
        "\n",
        "# # Search original and rewritten in parallel\n",
        "# orig_results = search_query(question, client, collection_name, co)\n",
        "# rewrite_results = search_query(rewritten_query, client, collection_name, co) if rewritten_query else []\n",
        "\n",
        "# # Merge and deduplicate\n",
        "# combined_context = list(dict.fromkeys(orig_results + rewrite_results))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvbcmighnehF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im67_ChcAiS-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eUa_yLY4AljD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_answer_groq(context, question, groq_api_key):\n",
        "    context_str = \"\\n\".join(context)\n",
        "    prompt = f\"\"\"Context:\n",
        "    {context_str}\n",
        "\n",
        "    Each context chunk ends with metadata (PDF name, URL, and updated date).\n",
        "    When answering, include relevant source(s) to justify your answer.\n",
        "\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {groq_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": \"llama3-8b-8192\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "        headers=headers,\n",
        "        json=data\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except KeyError:\n",
        "        print(\"❌ Error from Groq:\", response.status_code, response.text)\n",
        "        return \"❌ No valid answer returned.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLNtNn1vneWQ",
        "outputId": "49252f77-6d9b-4cfe-e0f7-b61294b6a079"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-38-744238724.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Answer: The limitation period for filing a writ petition is not explicitly mentioned in the given context. However, the context does mention the period of limitation for filing a reference application under Section 47 of the Sales Tax Act, 1990, which is 30 days from the communication of the order of the Appellate Tribunal or the Commissioner (Appeals).\n",
            "\n",
            "In the absence of specific information regarding the limitation period for filing a writ petition, it is necessary to consult other sources. According to the Civil Procedure Code (CPC), a writ petition is a special proceeding under Order 45, Rule 1 of the CPC. The CPC provides a limitation period of 90 days for filing a revision application (Order 47, Rule 1), which may be applied as a general guideline. However, the limitation period for filing a writ petition under Article 199 of the Constitution of Pakistan is not clearly defined in the CPC.\n",
            "\n",
            "In a landmark case, \"Asad Ali and 9 others vs. The Bank of Punjab and others\" (PLD 2020 SC 736), the Supreme Court of Pakistan held that the limitation period for filing a writ petition is 90 days from the date of the order or decision that is challenged.\n"
          ]
        }
      ],
      "source": [
        "# question = \"What is the limitation period for filing a writ petition?\"\n",
        "# context = combined_context\n",
        "# answer = generate_answer_groq(context, question, groq_api_key)\n",
        "\n",
        "# print(\"🤖 Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpAmXUC_iQLc",
        "outputId": "ac85cdf5-7dcb-4fa6-ed74-40222ddc5dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Rewritten Query: Here is the rewritten query:\n",
            "\n",
            "\"What is the limitation period for filing a writ petition?\"\n",
            "\n",
            "This query is concise and clear, and it directly confronts the user's question about the limitation period for filing a writ petition.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Kashif\\AppData\\Local\\Temp\\ipykernel_16428\\1955291120.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Answer: According to the Supreme Court's guidelines, the limitation period for filing a writ petition is 90 days from the date of the judgment or order that is being challenged. [1]\n",
            "\n",
            "Specifically, Rule 3 of Order XXXIX of the Code of Civil Procedure, 1908 states that a writ petition must be presented to the Supreme Court \"within sixty days from the date of the judgment or order complained of, or within sixty days from the date of the receipt of the notice or from the date of the happening of the event complained of, as the case may be\". [2]\n",
            "\n",
            "However, in the case of Apex Dental College & Hospital versus State of Rajasthan and others, the Supreme Court allowed the petition to be filed beyond the 60-day limitation period, considering it as an isolated case. [3]\n",
            "\n",
            "In summary, the general limitation period for filing a writ petition is 60 days from the date of the judgment or order complained of, but the Supreme Court may allow exceptions in specific cases.\n",
            "\n",
            "References:\n",
            "\n",
            "[1] Supreme Court of India. (2020). Guidelines for Filing of Writ Petitions. Retrieved from <https://main.sci.gov.in/pdf/Sample-Filing-Of-Writ-Petition.pdf>\n",
            "\n",
            "[2] Code of Civil Procedure, 1908. (Section 3, Order 39). Retrieved from <https://indiacode.nic.in>\n",
            "\n",
            "[3] Apex Dental College & Hospital vs State of Rajasthan & Others. (2005) 2 SCC 233.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the limitation period for filing a writ petition?\"\n",
        "conversation = f\"User: {question}\"\n",
        "\n",
        "# Rewrite user query\n",
        "rewritten_query = query_rewriter(conversation, groq_api_key)\n",
        "print(\"📝 Rewritten Query:\", rewritten_query)\n",
        "\n",
        "# Vector search using both original and rewritten\n",
        "orig_results = search_query(question, client, collection_name, co)\n",
        "rewrite_results = search_query(rewritten_query, client, collection_name, co) if rewritten_query else []\n",
        "combined_context = list(dict.fromkeys(orig_results + rewrite_results))\n",
        "\n",
        "# Rerank using Cohere\n",
        "context = rerank_context(question, combined_context, co)\n",
        "\n",
        "# Answer using LLM\n",
        "answer = generate_answer_groq(context, question, groq_api_key)\n",
        "print(\"🤖 Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2Y5jHoDBxCH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
