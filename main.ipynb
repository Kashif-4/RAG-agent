{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL\n",
    "BASE_URL = \"https://data.lhc.gov.pk/reported_judgments/judgments_approved_for_reporting\"\n",
    "\n",
    "# Folder to save PDFs\n",
    "SAVE_DIR = \"lhc_pdfs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the page\n",
    "response = requests.get(BASE_URL, timeout=30)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Failed to load page: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "pdf_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 PDFs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find all anchor tags with hrefs ending in .pdf\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    href = link[\"href\"]\n",
    "    if href.lower().endswith(\".pdf\"):\n",
    "        full_url = urljoin(BASE_URL, href)\n",
    "        pdf_links.append(full_url)\n",
    "\n",
    "print(f\"Found {len(pdf_links)} PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDFs: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download each PDF\n",
    "for pdf_url in tqdm(pdf_links, desc=\"Downloading PDFs\"):\n",
    "    filename = pdf_url.split(\"/\")[-1]\n",
    "    save_path = os.path.join(SAVE_DIR, filename)\n",
    "\n",
    "    try:\n",
    "        with requests.get(pdf_url, stream=True, timeout=30) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to download {pdf_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraxting text from pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.1-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.1-cp39-abi3-win_amd64.whl (18.5 MB)\n",
      "   ---------------------------------------- 0.0/18.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.5 MB 2.1 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/18.5 MB 2.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.6/18.5 MB 2.3 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.1/18.5 MB 2.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.4/18.5 MB 2.3 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.9/18.5 MB 2.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.1/18.5 MB 2.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.9/18.5 MB 2.3 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 4.2/18.5 MB 2.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 4.7/18.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.2/18.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.8/18.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 6.0/18.5 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 6.6/18.5 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 6.8/18.5 MB 2.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 7.1/18.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 7.6/18.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 7.9/18.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.4/18.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 8.9/18.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 9.7/18.5 MB 2.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.0/18.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 10.5/18.5 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 10.7/18.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 11.0/18.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 11.3/18.5 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 11.8/18.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.3/18.5 MB 2.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 12.6/18.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 13.1/18.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 13.6/18.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 14.2/18.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.7/18.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 15.2/18.5 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.7/18.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.3/18.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.8/18.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.0/18.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.0/18.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.0/18.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.0/18.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.3/18.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.5/18.5 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "PDF_DIR = \"lhc_pdfs\"\n",
    "EXTRACTED_DIR = \"extracted_texts\"\n",
    "os.makedirs(EXTRACTED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Extracting from: 2025LHC3591.pdf\n",
      "📄 Extracting from: 2025LHC3608.pdf\n",
      "📄 Extracting from: 2025LHC3612.pdf\n",
      "📄 Extracting from: 2025LHC3628.pdf\n",
      "📄 Extracting from: 2025LHC3636.pdf\n",
      "📄 Extracting from: 2025LHC3644.pdf\n",
      "📄 Extracting from: 2025LHC3661.pdf\n",
      "📄 Extracting from: 2025LHC3670.pdf\n",
      "📄 Extracting from: 2025LHC3677.pdf\n",
      "📄 Extracting from: 2025LHC3687.pdf\n",
      "📄 Extracting from: 2025LHC3697.pdf\n",
      "📄 Extracting from: 2025LHC3708.pdf\n",
      "📄 Extracting from: 2025LHC3719.pdf\n",
      "📄 Extracting from: 2025LHC3751.pdf\n",
      "📄 Extracting from: 2025LHC3760.pdf\n",
      "📄 Extracting from: 2025LHC3768.pdf\n",
      "📄 Extracting from: 2025LHC3791.pdf\n",
      "📄 Extracting from: 2025LHC3801.pdf\n",
      "📄 Extracting from: 2025LHC3823.pdf\n",
      "📄 Extracting from: 2025LHC3828.pdf\n",
      "📄 Extracting from: 2025LHC3836.pdf\n",
      "📄 Extracting from: 2025LHC3845.pdf\n",
      "📄 Extracting from: 2025LHC3866.pdf\n",
      "📄 Extracting from: 2025LHC3872.pdf\n",
      "📄 Extracting from: 2025LHC3883.pdf\n",
      "📄 Extracting from: 2025LHC3888.pdf\n",
      "📄 Extracting from: 2025LHC3892.pdf\n",
      "📄 Extracting from: 2025LHC3906.pdf\n",
      "📄 Extracting from: 2025LHC3913.pdf\n",
      "📄 Extracting from: 2025LHC3926.pdf\n",
      "📄 Extracting from: 2025LHC3932.pdf\n",
      "📄 Extracting from: 2025LHC3958.pdf\n",
      "📄 Extracting from: 2025LHC3979.pdf\n",
      "📄 Extracting from: 2025LHC3988.pdf\n",
      "📄 Extracting from: 2025LHC3999.pdf\n",
      "📄 Extracting from: 2025LHC4022.pdf\n",
      "📄 Extracting from: 2025LHC4030.pdf\n",
      "📄 Extracting from: 2025LHC4069.pdf\n",
      "📄 Extracting from: 2025LHC4075.pdf\n",
      "📄 Extracting from: 2025LHC4079.pdf\n",
      "📄 Extracting from: 2025LHC4120.pdf\n",
      "📄 Extracting from: 2025LHC4127.pdf\n",
      "📄 Extracting from: 2025LHC4141.pdf\n",
      "📄 Extracting from: 2025LHC4147.pdf\n",
      "📄 Extracting from: 2025LHC4155.pdf\n",
      "📄 Extracting from: 2025LHC4162.pdf\n",
      "📄 Extracting from: 2025LHC4176.pdf\n",
      "📄 Extracting from: 2025LHC4190.pdf\n",
      "📄 Extracting from: 2025LHC4204.pdf\n",
      "📄 Extracting from: 2025LHC4210.pdf\n"
     ]
    }
   ],
   "source": [
    "# Loop through PDFs and extract text\n",
    "for filename in os.listdir(PDF_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        path = os.path.join(PDF_DIR, filename)\n",
    "        print(f\"📄 Extracting from: {filename}\")\n",
    "        try:\n",
    "            text = extract_text_from_pdf(path)\n",
    "            # Save as .txt (optional)\n",
    "            with open(os.path.join(EXTRACTED_DIR, filename.replace(\".pdf\", \".txt\")), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifZ7uhK8Va1D0BsVBNUbwtH2dysmkph4j9RHbvX8\n",
    "\n",
    "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.3ufGsS34vNHGU_KvlSrSSv2LjsSLBLJ4q9J_z8BY_P8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Load Cohere\n",
    "co = cohere.Client(\"ifZ7uhK8Va1D0BsVBNUbwtH2dysmkph4j9RHbvX8\")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"https://4ae1bf46-a1be-419c-8f7a-751a29d868c2.eu-west-1-0.aws.cloud.qdrant.io\",  # from Qdrant Cloud\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.3ufGsS34vNHGU_KvlSrSSv2LjsSLBLJ4q9J_z8BY_P8\"                      # from API Keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kashif\\AppData\\Local\\Temp\\ipykernel_34276\\809953399.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recreated collection: lhc_judgments\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"lhc_judgments\"\n",
    "\n",
    "# ✅ Recreate the collection (automatically deletes if it exists)\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=384,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(f\"✅ Recreated collection: {collection_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors after recreation: 0\n"
     ]
    }
   ],
   "source": [
    "info = client.get_collection(collection_name=collection_name)\n",
    "print(f\"Vectors after recreation: {info.points_count}\")  # Should be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<qdrant_client.qdrant_client.QdrantClient object at 0x000001F520CCF770>\n"
     ]
    }
   ],
   "source": [
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in .\\venv\\lib\\site-packages (1.14.3)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in .\\venv\\lib\\site-packages (from qdrant-client) (1.73.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in .\\venv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in .\\venv\\lib\\site-packages (from qdrant-client) (2.3.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in .\\venv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in .\\venv\\lib\\site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in .\\venv\\lib\\site-packages (from qdrant-client) (2.11.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in .\\venv\\lib\\site-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: pywin32>=226 in .\\venv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (310)\n",
      "Requirement already satisfied: anyio in .\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in .\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in .\\venv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in .\\venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in .\\venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in .\\venv\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in .\\venv\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in .\\venv\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in .\\venv\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade qdrant-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks, chunk = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len((chunk + sentence).split()) > max_tokens:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += sentence + \". \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "texts_dir = Path(\"extracted_texts\")\n",
    "all_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in texts_dir.glob(\"*.txt\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    chunks = chunk_text(text)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\"text\": chunk, \"source\": file.name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "added delay bcz hitting limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_embeddings_with_retry(texts, retries=10, delay=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return co.embed(\n",
    "                texts=texts,\n",
    "                model=\"embed-english-light-v3.0\",\n",
    "                input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "        except Exception as e:\n",
    "            if hasattr(e, \"status_code\") and e.status_code == 429:\n",
    "                print(f\"⚠️ Rate limit hit (429). Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "            elif \"rate limit\" in str(e).lower():\n",
    "                print(f\"⚠️ Rate limit message. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise e\n",
    "    raise RuntimeError(\"❌ Failed after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploaded batch 1: 10 vectors\n",
      "📤 Uploaded batch 2: 10 vectors\n",
      "📤 Uploaded batch 3: 10 vectors\n",
      "📤 Uploaded batch 4: 10 vectors\n",
      "📤 Uploaded batch 5: 10 vectors\n",
      "📤 Uploaded batch 6: 10 vectors\n",
      "📤 Uploaded batch 7: 10 vectors\n",
      "📤 Uploaded batch 8: 10 vectors\n",
      "📤 Uploaded batch 9: 10 vectors\n",
      "📤 Uploaded batch 10: 10 vectors\n",
      "📤 Uploaded batch 11: 10 vectors\n",
      "📤 Uploaded batch 12: 10 vectors\n",
      "📤 Uploaded batch 13: 10 vectors\n",
      "📤 Uploaded batch 14: 10 vectors\n",
      "📤 Uploaded batch 15: 10 vectors\n",
      "📤 Uploaded batch 16: 10 vectors\n",
      "📤 Uploaded batch 17: 10 vectors\n",
      "📤 Uploaded batch 18: 10 vectors\n",
      "📤 Uploaded batch 19: 10 vectors\n",
      "📤 Uploaded batch 20: 10 vectors\n",
      "📤 Uploaded batch 21: 10 vectors\n",
      "📤 Uploaded batch 22: 10 vectors\n",
      "📤 Uploaded batch 23: 10 vectors\n",
      "📤 Uploaded batch 24: 10 vectors\n",
      "📤 Uploaded batch 25: 10 vectors\n",
      "📤 Uploaded batch 26: 10 vectors\n",
      "📤 Uploaded batch 27: 10 vectors\n",
      "📤 Uploaded batch 28: 10 vectors\n",
      "⚠️ Rate limit hit (429). Retrying in 10 seconds... (Attempt 1/10)\n",
      "📤 Uploaded batch 29: 10 vectors\n",
      "📤 Uploaded batch 30: 10 vectors\n",
      "📤 Uploaded batch 31: 10 vectors\n",
      "📤 Uploaded batch 32: 10 vectors\n",
      "📤 Uploaded batch 33: 10 vectors\n",
      "📤 Uploaded batch 34: 10 vectors\n",
      "📤 Uploaded batch 35: 10 vectors\n",
      "📤 Uploaded batch 36: 10 vectors\n",
      "📤 Uploaded batch 37: 10 vectors\n",
      "📤 Uploaded batch 38: 10 vectors\n",
      "📤 Uploaded batch 39: 9 vectors\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10 \n",
    "\n",
    "\n",
    "# ✅ Now your loop\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    batch = all_chunks[i:i+batch_size]\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "\n",
    "    embeddings = get_embeddings_with_retry(texts)\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=emb,\n",
    "            payload={\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"source\": chunk[\"source\"]\n",
    "            }\n",
    "        )\n",
    "        for emb, chunk in zip(embeddings, batch)\n",
    "    ]\n",
    "\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"📤 Uploaded batch {i // batch_size + 1}: {len(points)} vectors\")\n",
    "\n",
    "\n",
    "    time.sleep(2)  # Optional: still good to pause to avoid hitting limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors stored in Qdrant: None\n",
      "Expected vectors: 389\n"
     ]
    }
   ],
   "source": [
    "# After uploading all batches\n",
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "print(f\"Total vectors stored in Qdrant: {collection_info.vectors_count}\")\n",
    "print(f\"Expected vectors: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=389 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=True, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=False, unindexed_filtering_update=False, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={}\n",
      "✅ Total vectors stored in Qdrant: 389\n",
      "🔢 Expected vectors: 389\n"
     ]
    }
   ],
   "source": [
    "# Check how many vectors were actually stored\n",
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "print(\"✅ Collection info:\", collection_info)\n",
    "\n",
    "# This is the actual stored vector count\n",
    "count = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"✅ Total vectors stored in Qdrant: {count}\")\n",
    "print(f\"🔢 Expected vectors: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total vectors stored in Qdrant: 389\n",
      "🔢 Expected vectors: 389\n"
     ]
    }
   ],
   "source": [
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "print(f\"✅ Total vectors stored in Qdrant: {collection_info.points_count}\")\n",
    "print(f\"🔢 Expected vectors: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, client, collection_name, co, top_k=3):\n",
    "    # Embed the user query\n",
    "    query_embed = co.embed(\n",
    "        texts=[query],\n",
    "        model=\"embed-english-light-v3.0\",\n",
    "        input_type=\"search_query\"\n",
    "    ).embeddings[0]\n",
    "    \n",
    "    # Search in Qdrant\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embed,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # Return retrieved text chunks\n",
    "    return [hit.payload.get(\"text\", \"\") for hit in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_answer_groq(context, question, groq_api_key):\n",
    "    context_str = \"\\n\".join(context)\n",
    "    prompt = f\"Context:\\n{context_str}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"llama3-8b-8192\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Debug if 'choices' is missing\n",
    "    try:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except KeyError:\n",
    "        print(\"❌ Error from Groq API:\", response.status_code, response.text)\n",
    "        return \"Failed to get a valid response from the LLM.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kashif\\AppData\\Local\\Temp\\ipykernel_34276\\1983143931.py:10: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Answer: Based on the court's judgment, the limitation period for filing a writ petition is not explicitly stated. However, the court mentioned Article 120 of the Limitation Act, 1908, which prescribes a period of six years for filing a suit for declaration. This implies that if a writ petition is seeking a declaration, it would likely need to be filed within six years from the date of the cause of action.\n",
      "\n",
      "However, if the writ petition is seeking other relief, such as a direction to the authority to take a particular action, the limitation period may be different. In general, the limitation period for filing a writ petition is governed by the Rules of the Supreme Court and the High Court, which may vary depending on the nature of the petition and the relief sought.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the limitation period for filing a writ petition?\"\n",
    "context = search_query(question, client, collection_name, co)\n",
    "answer = generate_answer_groq(context, question, \"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\")\n",
    "\n",
    "print(\"🤖 Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
